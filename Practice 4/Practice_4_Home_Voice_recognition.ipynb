{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('voice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label = [1 if each ==\"female\" else 0 for each in data.label]\n",
    "data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.label.values\n",
    "x_data = data.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization\n",
    "#(x-min(x))/(max(x)-min(x))\n",
    "x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2534, 20)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_voices shape: (20, 2534)\n",
      "test_voices shape: (20, 634)\n",
      "train_labels shape: (2534,)\n",
      "test_labels shape: (634,)\n"
     ]
    }
   ],
   "source": [
    "#find transpose\n",
    "train_voices = x_train.T\n",
    "test_voices = x_test.T\n",
    "train_labels = y_train.T\n",
    "test_labels = y_test.T\n",
    "print(\"train_voices shape:\", train_voices.shape)\n",
    "print(\"test_voices shape:\", test_voices.shape)\n",
    "print(\"train_labels shape:\", train_labels.shape)\n",
    "print(\"test_labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_voices = train_voices.values\n",
    "test_voices = test_voices.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_labels))\n",
    "print(type(train_voices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    max_elem = np.max(x, axis = 0)\n",
    "    y = x - max_elem\n",
    "    y = np.exp(y)\n",
    "    sums = np.sum(y, axis = 0)\n",
    "    return y / sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y_label):\n",
    "    C = int(y_label.max() + 1)\n",
    "    enc = np.zeros((C, y_label.size))\n",
    "    enc[y_label.astype(int), np.arange(y_label.size)] = 1\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, sizes):\n",
    "        self.W0 = np.random.randn(sizes[1], sizes[0])\n",
    "        self.W1 = np.random.randn(sizes[2], sizes[1])\n",
    "        self.mW0 = np.zeros_like(self.W0)\n",
    "        self.mW1 = np.zeros_like(self.W1)\n",
    "        \n",
    "    def train(self, X, y_label, epochs = 10, alpha = 0.1, beta = 0.8):\n",
    "        y_one_hot = one_hot_encoding(y_label)\n",
    "        for epoch in range(epochs):\n",
    "            a0 = self.W0 @ X\n",
    "            z0 = np.maximum(a0, 0)\n",
    "            a1 = self.W1 @ z0\n",
    "            y = softmax(a1)\n",
    "            \n",
    "            loss = - (np.log(y + eps) * y_one_hot).sum() / y_label.size\n",
    "            acc = (np.argmax(y, axis = 0) == y_label).sum() / y_label.size\n",
    "            \n",
    "            d_a_1 = y - y_one_hot\n",
    "            gradW1 = d_a_1 @ z0.T\n",
    "            d_z_0 = self.W1.T @ d_a_1\n",
    "            d_a_0 = d_z_0.copy()\n",
    "            d_a_0[a0<0] = 0\n",
    "            gradW0 = d_a_0 @ X.T\n",
    "            \n",
    "            self.mW0 = beta * self.mW0 - alpha * gradW0\n",
    "            self.mW1 = beta * self.mW1 - alpha * gradW1\n",
    "            \n",
    "            self.W0 += self.mW0\n",
    "            self.W1 += self.mW1\n",
    "            \n",
    "            print(\"loss: \", loss, \" acc: \", acc * 100, \"%\")\n",
    "            \n",
    "    def stoch_train(self, X, y_label, epochs = 10, alpha = 0.1, beta = 0.8, mbSize = 100):\n",
    "        y_one_hot = one_hot_encoding(y_label)\n",
    "        iters_per_epoch = int(y_label.size / mbSize)\n",
    "        indices = np.arange(y_label.size)\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            X_sh = X[:, indices]\n",
    "            y_one_sh = y_one_hot[:, indices]\n",
    "            \n",
    "            for it in range(iters_per_epoch):\n",
    "                X_iter = X_sh[:, it*mbSize:(it+1)*mbSize]\n",
    "                y_one_iter = y_one_sh[:, it*mbSize:(it+1)*mbSize]\n",
    "                \n",
    "                \n",
    "            \n",
    "                a0 = self.W0 @ X_iter\n",
    "                z0 = np.maximum(a0, 0)\n",
    "                a1 = self.W1 @ z0\n",
    "                y = softmax(a1)\n",
    "\n",
    "\n",
    "                d_a_1 = y - y_one_iter\n",
    "                gradW1 = d_a_1 @ z0.T\n",
    "                d_z_0 = self.W1.T @ d_a_1\n",
    "                d_a_0 = d_z_0.copy()\n",
    "                d_a_0[a0<0] = 0\n",
    "                gradW0 = d_a_0 @ X_iter.T\n",
    "\n",
    "                self.mW0 = beta * self.mW0 - alpha * gradW0\n",
    "                self.mW1 = beta * self.mW1 - alpha * gradW1\n",
    "\n",
    "                self.W0 += self.mW0\n",
    "                self.W1 += self.mW1\n",
    "            loss, acc = self.evaluate(X, y_label)\n",
    "            print(\"loss: \", loss, \" acc: \", acc * 100, \"%\")\n",
    "    def evaluate(self, X, y_label):\n",
    "        y_one_hot = one_hot_encoding(y_label)\n",
    "        a0 = self.W0 @ X\n",
    "        z0 = np.maximum(a0, 0)\n",
    "        a1 = self.W1 @ z0\n",
    "        y = softmax(a1)\n",
    "        loss = - (np.log(y + eps) * y_one_hot).sum() / y_label.size\n",
    "        acc = (np.argmax(y, axis = 0) == y_label).sum() / y_label.size\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_net = NN([20, 70, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  3.1518889073553455  acc:  51.499605367008684 %\n",
      "loss:  2.8814925418336603  acc:  51.223362273086025 %\n",
      "loss:  2.4097241329520047  acc:  50.94711917916338 %\n",
      "loss:  1.8740865027810825  acc:  52.05209155485399 %\n",
      "loss:  1.504813549257675  acc:  56.55090765588003 %\n",
      "loss:  1.4782029768690204  acc:  61.2075769534333 %\n",
      "loss:  1.6918428005574349  acc:  59.94475138121547 %\n",
      "loss:  1.9174373543864598  acc:  57.14285714285714 %\n",
      "loss:  2.015552818396742  acc:  56.03788476716653 %\n",
      "loss:  1.9456343579726945  acc:  56.3930544593528 %\n",
      "loss:  1.734650729486629  acc:  58.445146014206784 %\n",
      "loss:  1.4578876240420473  acc:  62.70718232044199 %\n",
      "loss:  1.2208920147251718  acc:  67.20599842146804 %\n",
      "loss:  1.1150993812002745  acc:  66.4561957379637 %\n",
      "loss:  1.1502475538323864  acc:  63.733228097868974 %\n",
      "loss:  1.2426042294424464  acc:  61.523283346487766 %\n",
      "loss:  1.2937542112648848  acc:  60.339384372533544 %\n",
      "loss:  1.257234410756644  acc:  61.12865035516969 %\n",
      "loss:  1.1454955034093686  acc:  64.04893449092344 %\n",
      "loss:  1.011046071630422  acc:  67.91633780584057 %\n"
     ]
    }
   ],
   "source": [
    "voice_net.train(train_voices, train_labels, beta = 0.9, alpha = 1e-6, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.17676745361494206  acc:  96.32991318074191 %\n",
      "loss:  0.13755657500939633  acc:  96.32991318074191 %\n",
      "loss:  0.11609258621577659  acc:  96.40883977900553 %\n",
      "loss:  0.0942777943125705  acc:  97.31649565903709 %\n",
      "loss:  0.0932497492217442  acc:  97.47434885556433 %\n",
      "loss:  0.10241117713907182  acc:  97.47434885556433 %\n",
      "loss:  0.08121791637168671  acc:  97.51381215469614 %\n",
      "loss:  0.0839508307740768  acc:  97.86898184688239 %\n",
      "loss:  0.07869941723211259  acc:  97.47434885556433 %\n",
      "loss:  0.11510926694309137  acc:  95.26440410418311 %\n",
      "loss:  0.07590898166243018  acc:  97.86898184688239 %\n",
      "loss:  0.07203036044614956  acc:  97.79005524861878 %\n",
      "loss:  0.07481121196904743  acc:  97.55327545382794 %\n",
      "loss:  0.08050269645286295  acc:  96.92186266771901 %\n",
      "loss:  0.07894991972789789  acc:  97.94790844514601 %\n",
      "loss:  0.07165310372810221  acc:  97.39542225730071 %\n",
      "loss:  0.08430531044922306  acc:  96.48776637726914 %\n",
      "loss:  0.07336946828106833  acc:  98.10576164167324 %\n",
      "loss:  0.07181813356165788  acc:  97.47434885556433 %\n",
      "loss:  0.07565933302484755  acc:  97.43488555643252 %\n"
     ]
    }
   ],
   "source": [
    "voice_net.stoch_train(train_voices, train_labels, alpha = 1e-3, epochs = 20, beta = 0.9, mbSize = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.06133162137526981  acc:  97.79179810725552 %\n"
     ]
    }
   ],
   "source": [
    "loss, acc = voice_net.evaluate(test_voices, test_labels)\n",
    "print(\"loss: \", loss, \" acc: \", acc * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('voice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label = [1 if each ==\"female\" else 0 for each in data.label]\n",
    "data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.label.values\n",
    "x_data = data.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization\n",
    "#(x-min(x))/(max(x)-min(x))\n",
    "x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2534, 20)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_voices shape: (20, 2534)\n",
      "test_voices shape: (20, 634)\n",
      "train_labels shape: (2534,)\n",
      "test_labels shape: (634,)\n"
     ]
    }
   ],
   "source": [
    "#find transpose\n",
    "train_voices = x_train.T\n",
    "test_voices = x_test.T\n",
    "train_labels = y_train.T\n",
    "test_labels = y_test.T\n",
    "print(\"train_voices shape:\", train_voices.shape)\n",
    "print(\"test_voices shape:\", test_voices.shape)\n",
    "print(\"train_labels shape:\", train_labels.shape)\n",
    "print(\"test_labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_voices = train_voices.values\n",
    "test_voices = test_voices.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_labels))\n",
    "print(type(train_voices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    max_elem = np.max(x, axis = 0)\n",
    "    y = x - max_elem\n",
    "    y = np.exp(y)\n",
    "    sums = np.sum(y, axis = 0)\n",
    "    return y / sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y_label):\n",
    "    C = int(y_label.max() + 1)\n",
    "    enc = np.zeros((C, y_label.size))\n",
    "    enc[y_label.astype(int), np.arange(y_label.size)] = 1\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, sizes):\n",
    "        self.W0 = np.random.randn(sizes[1], sizes[0])\n",
    "        self.W1 = np.random.randn(sizes[2], sizes[1])\n",
    "        self.mW0 = np.zeros_like(self.W0)\n",
    "        self.mW1 = np.zeros_like(self.W1)\n",
    "        \n",
    "    def train(self, X, y_label, epochs = 10, alpha = 0.1, beta = 0.8):\n",
    "        y_one_hot = one_hot_encoding(y_label)\n",
    "        for epoch in range(epochs):\n",
    "            a0 = self.W0 @ X\n",
    "            z0 = np.maximum(a0, 0)\n",
    "            a1 = self.W1 @ z0\n",
    "            y = softmax(a1)\n",
    "            \n",
    "            loss = - (np.log(y + eps) * y_one_hot).sum() / y_label.size\n",
    "            acc = (np.argmax(y, axis = 0) == y_label).sum() / y_label.size\n",
    "            \n",
    "            d_a_1 = y - y_one_hot\n",
    "            gradW1 = d_a_1 @ z0.T\n",
    "            d_z_0 = self.W1.T @ d_a_1\n",
    "            d_a_0 = d_z_0.copy()\n",
    "            d_a_0[a0<0] = 0\n",
    "            gradW0 = d_a_0 @ X.T\n",
    "            \n",
    "            self.mW0 = beta * self.mW0 - alpha * gradW0\n",
    "            self.mW1 = beta * self.mW1 - alpha * gradW1\n",
    "            \n",
    "            self.W0 += self.mW0\n",
    "            self.W1 += self.mW1\n",
    "            \n",
    "            print(\"loss: \", loss, \" acc: \", acc * 100, \"%\")\n",
    "            \n",
    "    def stoch_train(self, X, y_label, epochs = 10, alpha = 0.1, beta = 0.8, mbSize = 100):\n",
    "        y_one_hot = one_hot_encoding(y_label)\n",
    "        iters_per_epoch = int(y_label.size / mbSize)\n",
    "        indices = np.arange(y_label.size)\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            X_sh = X[:, indices]\n",
    "            y_one_sh = y_one_hot[:, indices]\n",
    "            \n",
    "            for it in range(iters_per_epoch):\n",
    "                X_iter = X_sh[:, it*mbSize:(it+1)*mbSize]\n",
    "                y_one_iter = y_one_sh[:, it*mbSize:(it+1)*mbSize]\n",
    "                \n",
    "                \n",
    "            \n",
    "                a0 = self.W0 @ X_iter\n",
    "                z0 = np.maximum(a0, 0)\n",
    "                a1 = self.W1 @ z0\n",
    "                y = softmax(a1)\n",
    "\n",
    "\n",
    "                d_a_1 = y - y_one_iter\n",
    "                gradW1 = d_a_1 @ z0.T\n",
    "                d_z_0 = self.W1.T @ d_a_1\n",
    "                d_a_0 = d_z_0.copy()\n",
    "                d_a_0[a0<0] = 0\n",
    "                gradW0 = d_a_0 @ X_iter.T\n",
    "\n",
    "                self.mW0 = beta * self.mW0 - alpha * gradW0\n",
    "                self.mW1 = beta * self.mW1 - alpha * gradW1\n",
    "\n",
    "                self.W0 += self.mW0\n",
    "                self.W1 += self.mW1\n",
    "            loss, acc = self.evaluate(X, y_label)\n",
    "            print(\"loss: \", loss, \" acc: \", acc * 100, \"%\")\n",
    "    def evaluate(self, X, y_label):\n",
    "        y_one_hot = one_hot_encoding(y_label)\n",
    "        a0 = self.W0 @ X\n",
    "        z0 = np.maximum(a0, 0)\n",
    "        a1 = self.W1 @ z0\n",
    "        y = softmax(a1)\n",
    "        loss = - (np.log(y + eps) * y_one_hot).sum() / y_label.size\n",
    "        acc = (np.argmax(y, axis = 0) == y_label).sum() / y_label.size\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_net = NN([20, 70, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  11.557710370200141  acc:  49.21073401736385 %\n",
      "loss:  11.514598586984087  acc:  49.21073401736385 %\n",
      "loss:  11.401698716994071  acc:  49.21073401736385 %\n",
      "loss:  11.14492517142731  acc:  49.21073401736385 %\n",
      "loss:  10.598946265984614  acc:  49.21073401736385 %\n",
      "loss:  9.576335131286248  acc:  49.13180741910024 %\n",
      "loss:  8.010281738360646  acc:  49.09234411996843 %\n",
      "loss:  6.092933865155335  acc:  48.34254143646409 %\n",
      "loss:  4.146662978982139  acc:  44.317284925019734 %\n",
      "loss:  2.7643928778503137  acc:  37.72691397000789 %\n",
      "loss:  2.688532007492353  acc:  41.75217048145225 %\n",
      "loss:  3.5882641711017134  acc:  50.0394632991318 %\n",
      "loss:  4.580847070562602  acc:  51.53906866614049 %\n",
      "loss:  5.300035989404633  acc:  51.42067876874506 %\n",
      "loss:  5.678841924082444  acc:  51.34175217048145 %\n",
      "loss:  5.730951543299147  acc:  51.34175217048145 %\n",
      "loss:  5.488884276783666  acc:  51.46014206787688 %\n",
      "loss:  4.9920503522963156  acc:  52.05209155485399 %\n",
      "loss:  4.2944697292293315  acc:  52.1310181531176 %\n",
      "loss:  3.478773616022406  acc:  52.1310181531176 %\n"
     ]
    }
   ],
   "source": [
    "voice_net.train(train_voices, train_labels, beta = 0.9, alpha = 1e-6, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.22630269827254051  acc:  92.38358326756116 %\n",
      "loss:  0.11459997086907478  acc:  96.13259668508287 %\n",
      "loss:  0.10174352893781655  acc:  97.00078926598263 %\n",
      "loss:  0.09761086128771324  acc:  97.43488555643252 %\n",
      "loss:  0.09677611429700117  acc:  97.39542225730071 %\n",
      "loss:  0.08663537954397375  acc:  97.63220205209156 %\n",
      "loss:  0.08307151048229325  acc:  96.96132596685084 %\n",
      "loss:  0.08019438986485637  acc:  97.19810576164167 %\n",
      "loss:  0.07840529137965324  acc:  97.07971586424625 %\n",
      "loss:  0.07317925908913643  acc:  97.79005524861878 %\n",
      "loss:  0.06980792336756103  acc:  98.02683504340963 %\n",
      "loss:  0.06800243790175664  acc:  98.06629834254143 %\n",
      "loss:  0.06766959300220997  acc:  98.14522494080505 %\n",
      "loss:  0.06434749857887273  acc:  98.10576164167324 %\n",
      "loss:  0.06930509512786359  acc:  97.43488555643252 %\n",
      "loss:  0.06623029923431699  acc:  97.86898184688239 %\n",
      "loss:  0.06391637634786887  acc:  97.86898184688239 %\n",
      "loss:  0.07555009006189357  acc:  97.90844514601422 %\n",
      "loss:  0.06409175740240346  acc:  98.34254143646409 %\n",
      "loss:  0.0671000763177726  acc:  97.55327545382794 %\n"
     ]
    }
   ],
   "source": [
    "voice_net.stoch_train(train_voices, train_labels, alpha = 1e-3, epochs = 20, beta = 0.9, mbSize = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.06950761038732944  acc:  98.26498422712933 %\n"
     ]
    }
   ],
   "source": [
    "loss, acc = voice_net.evaluate(test_voices, test_labels)\n",
    "print(\"loss: \", loss, \" acc: \", acc * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
